Fine-tuning Transformer Models Using PEFT (Parameter-Efficient Fine-Tuning) Techniques


This notebook is part of a broader study on Parameter-Efficient Fine-Tuning (PEFT) methods. For a detailed explanation of the concepts and techniques covered here, check out:

English article: Parameter-Efficient Fine-Tuning (PEFT) Methods
Portuguese article: T√©cnicas de Fine-Tuning Eficiente em LLMs
Experimentation analysis: Ajuste Fino de Modelos Transformers atrav√©s de T√©cnicas PEFT
Visit the links to explore the theoretical context and decisions behind this code! üîç

Transformer Architecture
The Transformer architecture, introduced in 2017 in the paper "Attention is All You Need", revolutionized the field of natural language processing (NLP) and deep learning. Unlike recurrent and convolutional architectures, Transformers rely entirely on the attention mechanism, enabling more efficient parallelism and superior performance on sequence-based tasks.

Original Architecture
The Transformer architecture is composed of two main components: the Encoder and the Decoder. The Encoder is responsible for mapping an input into an intermediate representation, while the Decoder uses this intermediate representation to generate the output.

<img width="1778" height="1047" alt="image" src="https://github.com/user-attachments/assets/71f69a02-c3cb-4d2a-bee3-ea0bb74dd769" />

Attention Mechanism
A key characteristic of Transformer models is that they are built with special layers called attention layers. These layers instruct the model to focus specifically on certain words in the input sentence (and to pay less attention to others) when processing the representation of each word in its context.

The attention mechanism is central to the Transformer architecture and consists of three main components:

Query (Q): Represents the current input element that seeks to attend to other parts of the sequence.
Key (K) and Value (V): Represent the features of the sequence that will be compared to the Query.
Attention is computed as a dot product between Q and K, followed by Softmax normalization to obtain attention weights, which are then applied to the Values.

In the context of multi-head attention, this operation is performed multiple times in parallel, allowing the model to learn and capture different patterns of attention across various subspaces.

Encoder Block
The encoder receives an input and builds its internal representation (its features). This means the model is optimized to understand and capture the meaning of the input.

Each Encoder Block consists of two main sub-layers:

Multi-Head Self-Attention: Allows the model to attend to different parts of the input sequence simultaneously. The attention layer is composed of multiple heads that perform attention operations in parallel. Their outputs are then concatenated and projected into a single representation.
Feed-Forward Neural Network: Composed of two linear layers with a non-linear activation (ReLU) in between. The first layer expands the data dimensionality, while the second reduces it back to the original size.
Both sub-layers are wrapped with:

Layer Normalization , to stabilize and accelerate training.
Layer Normalization , to help preserve gradients and improve convergence during training.

Decoder Block
The decoder uses the representation generated by the encoder (features) along with other inputs to produce the target sequence. This means the model is optimized for output generation.

The Decoder is similar to the Encoder, but with a few key differences:

Masked Multi-Head Self-Attention: Similar to the encoder‚Äôs attention mechanism, but includes a mask that prevents the current position from attending to future positions ‚Äî essential for sequential generation tasks.
Multi-Head Attention: Performs attention over the encoder‚Äôs output, allowing the decoder to focus on the most relevant parts of the input sequence.
Feed-Forward Neural Network: Identical in structure to the one used in the encoder ‚Äî two linear layers with a ReLU activation in between.
As in the encoder, each sub-layer in the decoder is wrapped with:

Layer Normalization , to stabilize training.

Residual Connections , to facilitate gradient flow and improve training efficiency.

Model Variations
Each part of the architecture can be used independently, depending on the task:

Encoder-only models: good for tasks that require understanding of the input, such as sentence classification and named entity recognition. These models are typically characterized as having "bidirectional" attention and are often called auto-encoding models.
BERT
DistilBERT
RoBERTa
Decoder-only models: good for generative tasks, such as text generation. At each step, for a given word, the attention layers can only access words positioned before it in the sentence. These models are typically called auto-regressive models.
GPT
GPT-2
Encoder-decoder models or sequence-to-sequence models: good for generative tasks that require an input, such as summarization, translation, question answering, and generative responses
BART
T5

Fine-Tuning
Transformers are large and complex models, and increasing their size and the amount of pre-training data is a common strategy to improve performance. However, training these models from scratch is highly costly in terms of time, computational resources, and even environmentally, due to energy consumption. Therefore, sharing pre-trained models has become essential, allowing the community to reduce costs and leverage efficiency gains.

<img width="1817" height="585" alt="image" src="https://github.com/user-attachments/assets/2337b589-9f78-48cf-a625-2ba766780ec0" />

Transfer Learning is the technique that leverages pre-trained models on one task to be adapted to another specific task. In large language models (LLMs), such as GPT or BERT, this technique allows the knowledge acquired from large corpora to be reused and adjusted for specific tasks, such as text classification, machine translation, or question answering. This adaptation is done by fine-tuning the model in a supervised manner with new labeled data from the target task.

Fine-Tuning is the process of specializing a pre-trained model for a specific task. It adjusts the model's weights based on a smaller labeled dataset, while maintaining the general knowledge acquired during pre-training. This approach requires less data and resources, resulting in significant efficiency gains without compromising performance on the new task.

Methods
Fine-tuning processes can be broadly categorized into two types: Full Fine-Tuning and Parameter-Efficient Fine-Tuning (PEFT).

<img width="1781" height="936" alt="image" src="https://github.com/user-attachments/assets/e7a3df28-c0d8-4b6a-9499-75235e0f3f1c" />

Full Fine-Tuning
Training all parameters of a model for a specific task. One technique within this type of process is Instruction Tuning, which essentially uses a dataset with Prompt-Completion pairs. These can be questions and answers, or instructions and actions taken.
<img width="844" height="422" alt="image" src="https://github.com/user-attachments/assets/771451ef-d873-4ca1-8c46-600105953c31" />

Source: https://www.coursera.org/learn/generative-ai-with-llms/

The full fine-tuning process is highly effective in specializing LLMs. However, the fact that tuning occurs across all model parameters makes this process slow and expensive. The alternative developed was the PEFT processes.

Parameter-Efficient Fine-Tuning (PEFT)
Parameter-Efficient Fine-Tuning (PEFT) methods can be classified according to two main aspects: their conceptual structure (e.g., introducing new parameters or adjusting existing parameters) and their primary objective (minimizing memory footprint, storage efficiency, or reducing computational costs). These methods are divided into three major categories:

Additive Methods

Additive methods introduce new parameters into the base model, typically through small adapter layers or by adjusting a portion of the input embeddings (known as soft prompts). These methods are widely used and include:

Adapters: Small dense (fully connected) networks that are inserted after specific transformer sublayers, enabling adaptation to new tasks without the need to train all model parameters.
Soft Prompts: Fine adjustments applied directly to the model's input embeddings, facilitating adaptation to target tasks without modifying the model's internal parameters.
These methods are generally memory-efficient, as they reduce the size of gradients and optimizer states.

Selective Methods

Selective methods adjust only a fraction of the model's existing parameters. This can be done in different ways, such as:

Upper Layer Tuning: Focusing on adjusting only the upper layers of the network, leaving the lower layers intact.
Specific Parameter Tuning: Selective training of certain types of parameters, such as biases, while other parameters remain frozen.
Sparse Updates: Selection of a specific subset of parameters to be trained. While promising, this approach can be computationally more expensive due to the need to identify the most relevant parameters.
Despite savings in terms of trained parameters, selective methods can have high computational costs, especially in sparse configurations.

Reparameterization-Based Methods

Reparameterization-based methods reduce the number of trainable parameters by using low-rank representations, exploiting the redundancy present in neural networks. Some of the main methods include:

LoRA (Low-Rank Adaptation): Uses low-rank matrix decomposition to represent weight updates, resulting in an efficient form of fine-tuning.
Intrinsic SAID: Employs the Fastfood transform, a technique for representing low-rank updates in a computationally efficient manner.
These methods offer a significant reduction in the number of parameters to be trained, making them ideal for situations where storage efficiency and training time are critical.

Additional Points

Additive Methods: While they introduce new parameters, they can be more efficient in terms of overall memory, reducing the amount of gradients and optimizer states that need to be stored.
Selective Methods: While promising for reducing the number of trained parameters, they can be computationally intensive, particularly in cases of sparse updates.
Hybrid Methods: Combinations of ideas from different categories are often explored to maximize performance, leveraging the best of each approach.

<img width="1793" height="898" alt="image" src="https://github.com/user-attachments/assets/1d80583e-6039-4cb5-b60b-029d4d23282f" />

source: Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning" and the PEFT module

Experimentation: Fine-Tuning an LLM for Customer-Agent Dialogue Summarization
This experimentation involves the practice of fine-tuning methods on language models for the task of customer service conversation summarization, covering the entire machine learning cycle, from data preparation to hyperparameter tuning and evaluation.

Installation and Configuration
We need the following ü§ó Hugging Face libraries:

transformers contains an API for training models and many pre-trained models
tokenizers is automatically installed with transformers and "tokenizes" our data (i.e., converts text into sequences of numbers)
datasets contains a rich source of data, allowing dataset extraction from the hub and processing functions.
peft contains PEFT (Parameter-Efficient Fine-Tuning) methods to efficiently adapt large pre-trained models to various downstream applications without full fine-tuning
bitsandbytes provides simple ways to quantize a model to 8 and 4 bits.
huggingface-hub HuggingFace platform with models, datasets, and open-source applications, so we can share,
We also install wandb to automatically instrument our training.



